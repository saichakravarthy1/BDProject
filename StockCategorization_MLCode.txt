//dbfs:/FileStore/tables/zophkcg41468719226681/libsvm_sample_data.txt
///FileStore/tables/w88l18kj1468881990392/amazon_cells_labelled_clean5.csv
// /FileStore/tables/axbqz6h11470074001312/Stocks.txt
// /FileStore/tables/2xxrrkq81470074370869/Stocks.txt
// /FileStore/tables/rorfqmal1470087200564/Stocks.txt
import org.apache.spark.SparkContext
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.feature.{HashingTF, Tokenizer}
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.sql.Row


val reviewdata = sc.textFile("dbfs:/FileStore/tables/rorfqmal1470087200564/Stocks.txt");


val lines = reviewdata.map{line => val parts= line.split("\\^")
(parts(1).toDouble, parts(0))
}


val splits = lines.randomSplit(Array(0.6, 0.4), seed = 11L)
val trainsplit = splits(0).cache()
val testsplit = splits(1)


val training = sqlContext.createDataFrame(trainsplit.collect()).toDF("label","text")
val tokenizer = new Tokenizer()
  .setInputCol("text")
  .setOutputCol("words")
val hashingTF = new HashingTF()
  .setNumFeatures(500)
  .setInputCol(tokenizer.getOutputCol)
  .setOutputCol("features")
val lr = new LogisticRegression()
  .setMaxIter(10)
  .setRegParam(0.01)
val pipeline = new Pipeline()
  .setStages(Array(tokenizer, hashingTF, lr))



// Fit the pipeline to training documents.
val model = pipeline.fit(training)

// now we can optionally save the fitted pipeline to disk
model.save("/tmp/spark-logistic-regression-model28")

// we can also save this unfit pipeline to disk
pipeline.save("/tmp/unfit-lr-model28")

// and load it back in during production
val sameModel = PipelineModel.load("/tmp/spark-logistic-regression-model28")

// Prepare test documents, which are unlabeled (id, text) tuples.

/*
val test = sqlContext.createDataFrame(Seq(
  ("spark i j k"),
  ("l m n"),
  ("mapreduce spark"),
  ("apache hadoop")
)).toDF("text")
*/

val test = sqlContext.createDataFrame(testsplit.collect()).toDF("actualclass","text")

var actual_predicted : List[(Double, Double)] = List()
// Make predictions on test documents.
val predicted = model.transform(test)
  .select("actualclass", "text", "probability", "prediction")
  .collect()
  .foreach { case Row(actualclass: Double, text: String, prob: Vector, prediction: Double) => 
    println(s"($text) --> prob=$prob, prediction=$prediction")
    actual_predicted = actual_predicted :+((prediction,actualclass))
    //println("a")
  }
 
//predicted.collect().foreach(x=>println(x))
val a_p = sc.parallelize(actual_predicted)


//Evaluation metrics
val metrics = new BinaryClassificationMetrics(a_p)
val auROC = metrics.areaUnderROC()
val auPR = metrics.areaUnderPR()
println("Area under ROC = " + auROC + "Area under PR = " + auPR)